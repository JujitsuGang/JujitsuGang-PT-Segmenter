[![Tests](https://github.com/JujitsuGang/JujitsuGang-PT-Segmenter/actions/workflows/tests.yml/badge.svg)](https://github.com/JujitsuGang/JujitsuGang-PT-Segmenter/actions/workflows/tests.yml)\n[![Documentation Status](https://readthedocs.org/projects/JujitsuGang-PT-Segmenter/badge/?version=latest)](https://JujitsuGang-PT-Segmenter.readthedocs.io/en/latest/?badge=latest)\n\n# Brazilian Legal Text Segmenter\nThis project presents a Legal Text Segmenter for Portuguese-Brazilian language.\n\nThe segmentation problem is formalized here by a 4-multiclass token-wise classification problem. Each token can be classified as follows:\n\n|Class |Description |\n| :--- | :--- |\n|0 |No-op |\n|1 |Start of sentence |\n|2 |Start of noise sequence |\n|3 |End of noise sequence |\n\n\nIn a curated dataset, comprised of ground-truth legal text segments, Our Segmenter achieves higher Precision and Recall for the Class 1 (Segment) than other available popular segmentation tools, such as [NLTK](https://github.com/nltk/nltk), [SpaCy](https://github.com/explosion/spaCy), and [LexNLP](https://github.com/LexPredict/lexpredict-lexnlp), with the latter being suitable for segmenting legal texts. In the table below we compare these algorithms against our Segmenter, showing results for both estimated Precision and Recall by using over 2000 unseen curated examples:\n\n| Segmentation Method | Precision | Recall |\n|:--- |:--- |:--- |\n| NLTK (v3.7) | 12.8557% | 19.6128% |\n| SpaCy (v3.5.0) | 11.2524% | 22.2331% |\n| LexNLP (v2.2.1.0) | 24.4427% | 28.1971% |\n| Our Segmenter v1 (BERT-2) | 96.3277% | 94.3781% |\n| Our Segmenter v2 (BERT-4)* | **97.5417%** | **96.9480%** |\n\n*Coming soon.*\n\n\n---\n\n## Table of Contents\n1. [Model details](#model-details)\n1. [Inference](#inference)\n2. [Training](#training)\n2. [Trained models](#trained-models)\n3. [Installation](#installation)\n4. [Usage examples](#usage-examples)\n1. [Standard models (Torch format, Huggingface Transformers compatible)](#standard-models)\n2. [Quantization in ONNX format](#quantization-in-onnx-format)\n3. [Quantization in Torch JIT format](#quantization-in-torch-jit-format)\n4. [Noise subsegment removal](#noise-subsegment-removal)\n5. [Experimental results](#experimental-results)\n6. [Train data](#train-data)\n7. [Package tests](#package-tests)\n8. [License](#license)\n9. [Citation](#citation)\n\n\n---\n## Model details\n\n### Inference\nThe trained models are Transformer Encoders (BERT) and Bidirectional LSTM (Bi-LSTM), with varyinng number of hidden layers (transformer blocks), and with support to up to 1024 subword tokens for BERT models. Since legal texts may exceed this limit, the present framework pre-segment the text into possibly overlapping 1024 subword windows automatically in a moving window fashion, feeding them to the Transformer Encoder independently. The encoder output is then combined ("pooled"), and the final prediction for each token is finally derived.\n\nThe *pooling* operations can be one of the following:\n\n|Pooling | Description |\n| :--- | :--- |\n| Max | Keep maximal overlapping logits. |\n| Sum | Sum overlapping logits. |\n| Gaussian (default for Bi-LSTM) | Weight overlapping logits by a Gaussian distribution, centered at the middle of each moving window. |\n| Assymetric-Max (default for BERT) | Keep maximal overlapping logits for all classes except "No-op" (which gets the minimal overlapping logit instead). |\n\n### Training\nThe data labeling process is semi-automatic, employing several *ad-hoc* regular expressions.\n\n---\n\n## Trained models\nPretrained Segmenter models are downloaded by using the Fetcher API.\n\nThe default models loaded for each algorithm is:\n- *BERT*: `2_layer_6000_vocab_size_bert`.\n- *Bi-LSTM Model*: `512_hidden_dim_6000_vocab_size_1_layer_lstm`.\n- *Tokenizer*: `6000_subword_tokenizer`.\n\nNote that the `2_layer_6000_vocab_size_bert` already has its own built-in tokenizer, which happens to be identical to `6000_subword_tokenizer`. Hence, providing `6000_subword_tokenizer` for BERT segmenter is unnecessary, and will give the same results if done.\n\n---\n\n## Installation\nTo install this package:\n```bash\npython -m pip install "git+https://github.com/JujitsuGang/JujitsuGang-PT-Segmenter"\n```\n\nIf you plan to use optimized models in ONNX format, you need to install some optional dependencies:\n```bash\npython -m pip install "segmentador[optimize] @ git+https://github.com/JujitsuGang/JujitsuGang-PT-Segmenter"\n```\n\n---\n\n## Usage examples\n### Standard models\nWhen loading a model, pretrained Segmenter models are downloaded automatically and cached locally.\n\n#### BERTSegmenter\n```python\nimport segmentador\n\nsegmenter_bert = segmentador.BERTSegmenter(\n    device="cpu",  # or 'cuda' for GPU\n    inference_pooling_operation="assymetric-max",\n)\n\nsample_text = """\nPROJETO DE LEI N. 0123 (Da Sra. Alguém)\nDispõe de algo. O Congresso Nacional decreta:\nArtigo 1. Este projeto de lei não tem efeito.\n    a) Item de exemplo; b) Segundo item; ou c) Terceiro item.\nArtigo 2. Esta lei passa a vigorar na data de sua publicação.\n"""\n\nseg_result = segmenter_bert(sample_text, return_logits=True)\n\nprint(seg_result.segments)\n# [\n#     'PROJETO DE LEI N. 0123 ( Da Sra. Alguém )',\n#     'Dispõe de algo.',\n#     'O Congresso Nacional decreta :',\n#     'Artigo 1. Este projeto de lei não tem efeito.',\n#     'a ) Item de exemplo ;',\n#     'b ) Segundo item ; ou',\n#     'c ) Terceiro item.',\n#     'Artigo 2. Esta lei passa a vigorar na data de sua publicação.',\n# ]\n\nprint(seg_result.logits)\n# [[ 7.75678301  0.15893856 -2.88991857 -5.1139946 ]\n#  [10.15956116 -2.35737801 -3.08267331 -4.61426926]\n#  [10.86083889 -2.60591483 -4.09350395 -4.16544533]\n#  ...\n#  [ 9.71361065 -1.58287859 -3.04793835 -5.78309536]\n#  [ 2.31029105  7.32992315 -2.93384242 -7.3394866 ]]\n```\n\n#### LSTMSegmenter\n```python\nimport segmentador\n\nsegmenter_lstm = segmentador.LSTMSegmenter(\n    device="cpu",  # or 'cuda' for GPU\n    inference_pooling_operation="gaussian",\n)\n\nsample_text = """\nPROJETO DE LEI N. 0123 (Da Sra. Alguém)\nDispõe de algo. O Congresso Nacional decreta:\nArtigo 1. Este projeto de lei não tem efeito.\n    a) Item de exemplo; b) Segundo item; ou c) Terceiro item.\nArtigo 2. Esta lei passa a vigorar na data de sua publicação.\n"""\n\nseg_result = segmenter_lstm(sample_text, return_logits=True)\n\nprint(seg_result.segments)\n# [\n#    'PROJETO DE LEI N. 0123 ( Da Sra. Alguém )',\n#    'Dispõe de algo.',\n#    'O Congresso Nacional decreta :',\n#    'Artigo 1. Este projeto de lei não tem efeito.',\n#    'a ) Item de exemplo ;',\n#    'b ) Segundo item ; ou',\n#    'c ) Terceiro item.',\n#    'Artigo 2. Esta lei passa a vigorar na data de sua publicação.',\n# ]\n\nprint(seg_result.logits)\n# [[  6.2647295   -8.58741379   5.64134645  -7.10431194]\n#  [  7.73504782  -2.77080107  -5.28328753 -10.26550961]\n#  [ 10.03150749  -7.33715487  -5.94148588  -7.88663769]\n#  ...\n#  [  6.64764452  -2.28969622  -3.06246185  -8.4958601 ]\n#  [ -0.75093395   5.79272366   2.84845114  -8.5399065 ]]\n```\n\n#### Local files or Huggingface HUB models\nYou can also provide local models (or compatible Huggingface HUB models) to initialize the segmenter model weights, by providing the `uri_model` and `uri_tokenizer` arguments. Remember that BERT models often have their own tokenizer built-in, wheres LSTM models do not. Therefore, providing a tokenizer model for LSTM models is a requirement, whereas for BERT models is optional.\n```python\nsegmenter_bert = segmentador.BERTSegmenter(\n    uri_model='<path_to_local_model_or_hf_hub_model_name>',\n    uri_tokenizer=None,\n)\n\nsegmenter_lstm = segmentador.LSTMSegmenter(\n    uri_model='<path_to_local_model>',\n    uri_tokenizer='<path_to_model_tokenizer>',\n)\n```\n\n---\n\n### Quantization in ONNX format\nWe provide support for models in ONNX format (and also functions to convert from pytorch to such format), which are highly optimized and also support weight quantization. We apply 8-bit dynamic quantization. Effects of quantization in segmenter models are analyzed in notebook.\n\nFirst, in order to use models in ONNX format you need to install some optional dependencies. Then, you need to create the ONNX quantized model using the `segmentador.optimize` subpackage API:\n\n```python\nimport segmentador.optimize\n\n# Load BERT Torch model\nsegmenter_bert = segmentador.BERTSegmenter()\n\n# Create ONNX BERT model\nquantized_model_paths = segmentador.optimize.quantize_model(\n    segmenter_bert,\n    model_output_format="onnx",\n    verbose=True,\n)\n```\n\nLastly, load the optimized models with appropriate classes from `segmentador.optimize` module. While the ONNX segmenter model configuration may differ from their standard (Torch format) version, its inference usage remains the same:\n\n```python\n# Load ONNX model\nsegmenter_bert_quantized = segmentador.optimize.ONNXBERTSegmenter(\n    uri_model=quantized_model_paths.output_uri,\n    uri_tokenizer=segmenter_bert.tokenizer.name_or_path,\n)\n\nseg_result = segmenter_bert_quantized(sample_text, return_logits=True)\n```\n\nThe procedure shown above is analogous for ONNX Bi-LSTM models:\n\n```python\nimport segmentador.optimize\n\n# Load Bi-LSTM standard model\nsegmenter_lstm = segmentador.LSTMSegmenter()\n\n# Create ONNX Bi-LSTM model\nquantized_lstm_paths = segmentador.optimize.quantize_model(\n    segmenter_lstm,\n    model_output_format="onnx",\n    verbose=True,\n)\n\n# Load ONNX model\nsegmenter_lstm_quantized = segmentador.optimize.ONNXLSTMSegmenter(\n    uri_model=quantized_lstm_paths.output_uri,\n    uri_tokenizer=segmenter_lstm.tokenizer.name_or_path,\n)\n\nseg_result = segmenter_lstm_quantized(curated_df_subsample, return_logits=True)\n```\n\n### Quantization in Torch JIT format\n\nModels can also be quantized as Torch JIT format:\n```Python\n# LSTM models quantized as Torch JIT format\nquantized_lstm_torch_paths = segmentador.optimize.quantize_model(\n    segmenter_lstm,\n    model_output_format="torch_jit",\n    verbose=True,\n)\n\nsegmenter_lstm_torch_quantized = segmentador.optimize.TorchJITLSTMSegmenter(\n   uri_model=quantized_lstm_torch_paths.output_uri,\n)\n\nseg_result = segmenter_lstm_torch_quantized(sample_text, return_logits=True)\n...\n```\n\n```Python\n# BERT models quantized as Torch JIT format\nquantized_bert_torch_paths = segmentador.optimize.quantize_model(\n    segmenter_bert,\n    model_output_format="torch_jit",\n    verbose=True,\n)\n\nsegmenter_bert_torch_quantized = segmentador.optimize.TorchJITBERTSegmenter(\n   uri_model=quantized_bert_torch_paths.output_uri,\n)\n\nseg_result = segmenter_bert_torch_quantized(sample_text, return_logits=True)\n...\n```\n\n### Noise subsegment removal\nTokens are classified as one out of 4 available classes: No-op (0), Segment (1), Noise Start (2), and Noise End (3). Tokens between a pair of `Noise Start` (inclusive) and the closest `Noise End` or `Segment` (either exclusive) can be removed during the segmentation, by passing the argument `remove_noise_subsegments=True` to the segmenter model:\n\n```python\nseg_result = segmenter(sample_text, remove_noise_subsegments=True)\n```\n\n---\n\n## Experimental results\nExperimental results are showed in [Result Analsys notebook](./notebooks/6_result_analysis.ipynb), with models tipically achieving per-class precision and recall higher than 95%, despite the problem being severely imbalanced. This same notebook also showcase some tests varying moving window size, moving window shift size, and Bidirectional LSTM models for comparison.\n\n---\n\n## Train data\nTODO.\n\n---\n\n## Package tests\nTests for this package are run using Tox and Pytest.\n\n---\n\n## License\n```markdown\nMIT License\n\nCopyright (c) 2022 Jujitsu Gang\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n```\n\n\n---\n\n## Citation\n```bibtex\n@inproceedings{\n    paper="",\n    author="",\n    date="",\n}\n```